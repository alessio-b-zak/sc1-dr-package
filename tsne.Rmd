---
title: "R Notebook"
output: html_notebook
---

We will do this analysis with the Iris dataset

```{r}
library(datasets)
data(iris)
ds <- as.matrix(iris[,1:3])
```



t-SNE


To calculate tSNE we first need a matrix of pairwise distances for our data points. As we are working with matrices a vectorised way we can do this is using the second binomial formula


```{r}

pairwise_dist <- function(X) {
  xbar <- rowSums(X*X)
  D <- (xbar %o% rep(1, length(xbar))) + (rep(1, length(xbar)) %o% xbar) - 2*(X %*% t(X))
  return(D)
}
```



```{r}
#return vector of pj|i for all j for a specific i and a given sigma_i
calculate_cond_probs_i <- function(pairwise_distances, sigma) {
  numerator_pj_g_i <- exp(-pairwise_distances * sigma)
  denominator_pj_g_i <- sum(numerator_pj_g_i)
  pj_g_i <- numerator_pj_g_i / denominator_pj_g_i
  print(pj_g_i)
  entropy <- -sum(log(pj_g_i)*pj_g_i)
  print(entropy)
  conds_prob_entropy <- list(entropy, pj_g_i)
  names(conds_prob_entropy) <- c("entropy", "p")
  return(conds_prob_entropy)
}
```

laurens calculation
```{r}
cond_probs <- function(pairwise_distances, sigma) {
  P <- exp(-pairwise_distances * sigma)
  sumP <- sum(P)
  H <- log(sumP) + (sigma * sum(pairwise_distances * P)) / sumP
  P <- P / sumP
  conds_prob_entropy <- list(H, P)
  names(conds_prob_entropy) <- c("entropy", "p")
  return(conds_prob_entropy)
}
```

One that hopefully doesn't underflow

```{r}
calculate_cond_probs_i <- function(pairwise_distances, sigma) {
  numerator_pj_g_i <- exp(-pairwise_distances * sigma)
  denominator_pj_g_i <- sum(numerator_pj_g_i)
  pj_g_i <- numerator_pj_g_i / denominator_pj_g_i
  entropy <- sigma * sum(pairwise_distances * pj_g_i) /  denominator_pj_g_i + log(denominator_pj_g_i)
  conds_prob_entropy <- list(entropy, pj_g_i)
  names(conds_prob_entropy) <- c("entropy", "p")
  return(conds_prob_entropy)
}
```



```{r}
alpha <- matrix(c(1,2,3,4), 2)
cond_probs(alpha,2)
calculate_cond_probs_i(alpha,2)
```


```{r}
binary_search_perplexity <- function(desired_perplexity, distance_col, tolerance=1e-5) {
  log_perplexity <- log(desired_perplexity)
  current_sigma <- 1
  max_sigma <- NA
  min_sigma <- NA
  
  probsi <- cond_probs(distance_col, current_sigma)
  current_entropy <-  probsi$entropy
  diff_entropy <- current_entropy - log_perplexity
  tries <- 0
  while((abs(diff_entropy)>tolerance) && (tries < 50)) {
    
    if(diff_entropy > 0) {
      min_sigma <- current_sigma
      if(is.na(min_sigma) || is.na(max_sigma)) {
        current_sigma <- current_sigma * 2
      }
      else{
        current_sigma <- (current_sigma + max_sigma) /2
      }
    }
    else {
      max_sigma <- current_sigma
      if(is.na(min_sigma) || is.na(max_sigma)) {
          current_sigma <- current_sigma / 2
      }
      else{
        current_sigma <- (current_sigma + min_sigma) /2
      }
    }
    probsi <- cond_probs(distance_col, current_sigma)
    current_entropy <-  probsi$entropy
    diff_entropy <- current_entropy - log_perplexity
    tries <-  tries + 1
  }
 return(probsi$p) 
}
```

```{r}
calculate_conditional_probabilities <- function(X, desired_perplexity) {
  pairwise_distances <- pairwise_dist(X)
  p <- apply(pairwise_distances, 1, function(x){binary_search_perplexity(desired_perplexity, x)})
  p  
}
```

```{r}
calculate_conditional_probabilities(ds, 2)
```

calculate p_i,j
```{r}
compute_p_ij <- function(pmat) {
  N <- dim(pmat)[1]
  return((pmat + t(pmat))/(2*N))
}
```


mapping probability functions for t-distribution
```{r}
compute_q_ij <- function(q_squared_distances) {
  qdim <- dim(q_squared_distances)[1]
  Q <- matrix(,qdim, qdim) 
  q_denom_sum <- 0
  for(i in 1:Q) {
    for(j in 1:Q) {
      q <- (1/(1 + q_squared_distances[i,j]))
      Q[i,j]  <- q
      q_denom_sum <-  q_denom_sum + q
    }
  }
  return(Q / q_denom_sum)
}
```


kullback-leibler cost

```{r}
kl_divergence <- function(P, Q) {
  P * log(P/Q)
}
```


```{r}
KL_grad_cost <- function(Y, i, P,Q, exaggeration) {
 gradient <- vector(mode="numerical", length=dim(Y)[1])
 
 
 
}
```

